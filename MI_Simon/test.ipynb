{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from customtkinter import *\n",
    "import pandas as pd\n",
    "import SupportFunctions\n",
    "import threading\n",
    "import queue\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "\n",
    "logger_filepath = SupportFunctions.get_data_file_path(filename='scrape_app.log', folder='database\\logs')\n",
    "logging.basicConfig(filename=logger_filepath, level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get the directory path of main.py\n",
    "current_dir = r\"D:\\Users\\Arian\\Documents\\Kubrick\\Projects\\Internal MI Copmetitor Profiling\\save_data\\MI_Simon\"\n",
    "metadata_filepath = SupportFunctions.get_data_file_path(filename = 'company_metadata.csv')\n",
    "\n",
    "# Read in metadata file\n",
    "company_df = pd.read_csv(filepath_or_buffer=metadata_filepath, index_col = 0)\n",
    "company_df.fillna('',inplace=True)\n",
    "# Filter to exclude not yet completed companies\n",
    "mask = company_df['scraper'] != 'to complete'\n",
    "\n",
    "df_full_company_list = company_df[['company_name','ticker']].fillna('')\n",
    "company_df = company_df[mask]\n",
    "full_company_list = dict(zip(df_full_company_list['company_name'], df_full_company_list['ticker']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapers\n",
    "\n",
    "df1 = scrapers.scraper_epam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed directory: d:\\Users\\Arian\\Documents\\Kubrick\\Projects\\Internal MI Copmetitor Profiling\\save_data\\MI_Simon\\database\\output\\EPAM Systems Inc\\EPAM Systems Inc-webscrape-June-2024.csv\n",
      "d:\\Users\\Arian\\Documents\\Kubrick\\Projects\\Internal MI Copmetitor Profiling\\save_data\\MI_Simon\\database\\output\\EPAM Systems Inc\\EPAM Systems Inc-webscrape-June-2024.csv\n"
     ]
    }
   ],
   "source": [
    "# company_name = 'EPAM Systems Inc'\n",
    "# company_url, scraper, status, ticker = SupportFunctions.get_company_metadata(company_name, company_df)\n",
    "# sanitized_name = re.sub(r'[<>:\"/\\\\|?*]', '_', company_name).strip()\n",
    "# file_path = SupportFunctions.get_data_file_path(f'*webscrape*.csv', f'database\\output\\{sanitized_name}')\n",
    "# file_path = glob.glob(file_path)[0]\n",
    "# midiff_file_path = SupportFunctions.get_data_file_path(f'{sanitized_name} diff.csv', f'database\\output\\{sanitized_name}')\n",
    "# template_file_path = SupportFunctions.get_data_file_path('Template.csv')\n",
    "# yahoo_json = SupportFunctions.get_company_details2(status, ticker)\n",
    "# final_df1 = SupportFunctions.create_final_df2(company_name, company_url, status, yahoo_json, df1, template_file_path)\n",
    "# # final_df1.loc[0, 'Practices'] = 'Testing'\n",
    "# old_df = SupportFunctions.read_from_csv(file_path, company_name, template_file_path)\n",
    "# SupportFunctions.log_new_and_modified_rows3(final_df1, old_df, midiff_file_path, company_name)\n",
    "# SupportFunctions.write_to_csv(final_df1, file_path, sanitized_name)\n",
    "\n",
    "\n",
    "\n",
    "company_name = 'EPAM Systems Inc'\n",
    "company_url, scraper, status, ticker = SupportFunctions.get_company_metadata(company_name, company_df)\n",
    "sanitized_name = re.sub(r'[<>:\"/\\\\|?*]', '_', company_name).strip()\n",
    "file_path = SupportFunctions.get_data_file_path(f'*webscrape*.csv', f'database\\output\\{sanitized_name}')\n",
    "file_path = glob.glob(file_path)[0]\n",
    "midiff_file_path = SupportFunctions.get_data_file_path(f'{sanitized_name} diff.csv', f'database\\output\\{sanitized_name}')\n",
    "template_file_path = SupportFunctions.get_data_file_path('Template.csv')\n",
    "yahoo_json = SupportFunctions.get_company_details2(status, ticker)\n",
    "final_df1 = SupportFunctions.create_final_df2(company_name, company_url, status, yahoo_json, df1, template_file_path)\n",
    "# final_df1.loc[0, 'Practices'] = 'Testing'\n",
    "old_df = SupportFunctions.read_from_csv(file_path, company_name, template_file_path)\n",
    "SupportFunctions.reallocate_old_df(old_df, file_path, sanitized_name)\n",
    "SupportFunctions.log_new_and_modified_rows3(final_df1, old_df, midiff_file_path, company_name)\n",
    "SupportFunctions.write_to_csv(final_df1, file_path, sanitized_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_differences(message, company_name, count):\n",
    "    # Dummy function to simulate logging\n",
    "    print(f\"{company_name}: {message} - {count} rows\")\n",
    "def abc(final_df, old_df, path_file, company_name):\n",
    "    \"\"\"\n",
    "    Compare rows between final_df and old_df, update or remove rows accordingly, \n",
    "    and handle Excel file updates.\n",
    "\n",
    "    Args:\n",
    "    final_df (pd.DataFrame): The most up-to-date dataframe.\n",
    "    old_df (pd.DataFrame): The older dataframe to compare against.\n",
    "    path_file (str): Path to save the CSV file.\n",
    "    company_name (str): Name of the company for logging purposes.\n",
    "    \"\"\"\n",
    "    # Subset to relevant columns\n",
    "    columns_of_interest = ['Practices', 'Practices_URL', 'Services', 'Services_URL', 'Solutions', 'Solutions_URL']\n",
    "    df1 = final_df[columns_of_interest].copy()\n",
    "    df2 = old_df[columns_of_interest].copy()\n",
    "\n",
    "    # Convert columns to the same data type (string) for comparison\n",
    "    for column in columns_of_interest:\n",
    "        df1[column] = df1[column].astype(str)\n",
    "        df2[column] = df2[column].astype(str)\n",
    "\n",
    "    # Initialize the temporary DataFrame to store unmatched rows with their status\n",
    "    temp_df = pd.DataFrame(columns=['Status'] + list(final_df.columns))\n",
    "\n",
    "    # Keep track of original indices before dropping rows\n",
    "    original_idx1 = df1.index.tolist()\n",
    "    original_idx2 = df2.index.tolist()\n",
    "\n",
    "    # Compare each row in df1 with each row in df2\n",
    "    for idx1, row1 in df1.iterrows():\n",
    "        matched = False\n",
    "        for idx2, row2 in df2.iterrows():\n",
    "            if row1.equals(row2):\n",
    "                # If a match is found, remove the row from both dataframes\n",
    "                df1.drop(idx1, inplace=True)\n",
    "                df2.drop(idx2, inplace=True)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            # If no match is found, mark the row as 'Added' and store in temp_df\n",
    "            full_row1 = final_df.loc[original_idx1[idx1]].copy()\n",
    "            full_row1['Status'] = 'Added'\n",
    "            temp_df = pd.concat([temp_df, pd.DataFrame([full_row1])], ignore_index=True)\n",
    "\n",
    "    # Any remaining rows in df2 are 'Removed'\n",
    "    for idx2, row2 in df2.iterrows():\n",
    "        full_row2 = old_df.loc[original_idx2[idx2]].copy()\n",
    "        full_row2['Status'] = 'Removed'\n",
    "        temp_df = pd.concat([temp_df, pd.DataFrame([full_row2])], ignore_index=True)\n",
    "\n",
    "    # If there are any changes, write them to the CSV file\n",
    "    if not temp_df.empty:\n",
    "        temp_df.to_csv(path_file, index=False)\n",
    "        log_differences(\"Changed\", company_name, len(temp_df))\n",
    "    else:\n",
    "        # If there are no new or modified rows, delete the CSV file if it exists\n",
    "        if os.path.exists(path_file):\n",
    "            os.remove(path_file)\n",
    "        log_differences(\"No differences found\", company_name, 0)\n",
    "abc(final_df1, old_df, midiff_file_path, company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reallocate_olddf(old_df, file_path, sanitized_name):\n",
    "    history_file_path = SupportFunctions.get_data_file_path(os.path.basename(file_path), f'database\\history\\{sanitized_name}')\n",
    "    # Remove the directory containing the csv_file\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f'Removed directory: {file_path}')\n",
    "    else:\n",
    "        print(f'Directory does not exist: {file_path}')\n",
    "    old_df.to_csv(history_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe saved to D:\\Users\\Arian\\Documents\\Kubrick\\Projects\\Internal MI Copmetitor Profiling\\save_data\\MI_Simon\\database\\output\\summary_diff_files.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def collect_diff_csv_files(main_directory):\n",
    "    # Initialize an empty dataframe\n",
    "    main_df = pd.DataFrame()\n",
    "    \n",
    "    # Walk through each directory and subdirectory\n",
    "    for root, dirs, files in os.walk(main_directory):\n",
    "        for file in files:\n",
    "            # Check if the file is a csv and contains 'diff' in its name\n",
    "            if file.endswith('.csv') and 'diff' in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Read the CSV file and append it to the main dataframe\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                main_df = pd.concat([main_df, temp_df], ignore_index=True)\n",
    "    \n",
    "    # If the main dataframe is not empty, save it to the original path directory\n",
    "    if not main_df.empty:\n",
    "        output_file = os.path.join(main_directory, f'summary_diff_files_.csv')\n",
    "        main_df.to_csv(output_file, index=False)\n",
    "        print(f'Merged dataframe saved to {output_file}')\n",
    "    else:\n",
    "        print('No diff CSV files found.')\n",
    "\n",
    "collect_diff_csv_files(r'D:\\Users\\Arian\\Documents\\Kubrick\\Projects\\Internal MI Copmetitor Profiling\\save_data\\MI_Simon\\database\\output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
